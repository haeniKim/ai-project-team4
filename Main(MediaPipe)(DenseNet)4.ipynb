{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성별/나이에 따른 엘리베이터 맞춤 광고 서비스 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1) 엘리베이터에 승객이 탑승하고 문이 닫힐 시 카메라 실행 (웹캠 실행)\n",
    "* 2) 카메라 촬영 후 안면 이미지 검출 (1명,2명,3명 이상 등)\n",
    "* 3) 검출된 안면 이미지별 성별/나이 예측\n",
    "* 4) 예측된 정보를 바탕으로 맞춤형 광고 송출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save images 1 second...\n",
      "Found 3 validated image filenames.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001AC1C7D7AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# MeidaPipe 모델 선언\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Camera open failed\")\n",
    "    exit()\n",
    "\n",
    "# 엘리베이터 상황 가정 : 문이 열린 후,닫힐 때마다 사진 촬영 (10초마다 촬영하는 것으로 가정)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't read camera\")\n",
    "        break\n",
    "\n",
    "    # 현재 시간\n",
    "    now = datetime.now()\n",
    "    now_path = 'log_'+str(now.year) + '_' + str(now.month) + '_' + str(now.day) + '_' + str(now.hour) + '_' + str(now.minute) + '_' + str(now.second) +'/'\n",
    "\n",
    "    \n",
    "    # 웹캠에서 프레임 읽기\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't read camera\")\n",
    "        break\n",
    "    \n",
    "    # (480,640,3)\n",
    "    # print(frame.shape)\n",
    "    # break\n",
    "\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        # MediaPipe를 활용한 안면 검출을 위해 BGR에서 RGB로\n",
    "        results = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # results.detections : 검출된 1개 이상의 안면 이미지지\n",
    "        if results.detections:\n",
    "            \n",
    "            # 안면 검출 이미지 리스트\n",
    "            cropped_images = [] \n",
    "            zoom_factor = 0.1\n",
    "            for face_id, detection in enumerate(results.detections):\n",
    "\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                # 안면 검출된 좌표 계산\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "                # 전체 이미지(frame)에서 얼굴 이미지 슬라이싱\n",
    "                face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "                print(\"origin size : \",face_roi.shape)\n",
    "\n",
    "                # 슬라이싱 확대하기 (CNN 학습데이터와 유사한 Margin)\n",
    "                expand_width = int(face_roi.shape[0]*0.3)\n",
    "                expand_height = int(face_roi.shape[1]*0.3)\n",
    "                # expand_height = int(face_roi.shape[1]*0.15)\n",
    "\n",
    "                face_roi = frame[np.clip(y-expand_height,0,480):np.clip(y+h+expand_height,0,480),np.clip(x-expand_width,0,640):np.clip(x + w+expand_width,0,640)]\n",
    "\n",
    "                print(\"convert size : \",face_roi.shape)\n",
    "\n",
    "                # CNN Multi-Label에 input하기 위한 resize\n",
    "                resized_face = cv2.resize(face_roi, (224, 224),fx=zoom_factor, fy=zoom_factor, interpolation=cv2.INTER_LINEAR)\n",
    "                cropped_images.append(resized_face)\n",
    "\n",
    "    root_save_path = 'WebCAM_MediaPipe/'\n",
    "\n",
    "    # 캡처 원본 사진 저장\n",
    "    cv2.imwrite(root_save_path+now_path+'_origin_frame.jpg', frame)\n",
    "\n",
    "    people_num = 0\n",
    "\n",
    "    try : \n",
    "        # 동일 사진에서 검출된 안면 이미지들을 동일 폴더에 저장\n",
    "        for idx, cropped_image in enumerate(cropped_images):\n",
    "            people_num += 1\n",
    "            resized_cropped_image = cv2.resize(cropped_image, (224, 224))\n",
    "            # resized_cropped_image_uint8 = (resized_cropped_image * 255).astype(np.uint8)\n",
    "\n",
    "            isExist = os.path.exists(root_save_path+now_path)\n",
    "            if not isExist:\n",
    "                os.makedirs(root_save_path+now_path)\n",
    "            output_filename = root_save_path+now_path + '_face_' + str(idx+1) + '.jpg' \n",
    "            # print(output_filename)\n",
    "            # cv2.imwrite(output_filename, resized_cropped_image_uint8,[int(cv2.IMWRITE_JPEG_QUALITY), 0])\n",
    "            cv2.imwrite(output_filename, resized_cropped_image)\n",
    "\n",
    "        from tensorflow.keras.models import load_model\n",
    "        from keras_preprocessing.image import ImageDataGenerator\n",
    "        from glob import glob\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import time\n",
    "\n",
    "        model = load_model(\"C:/Users/user/Desktop/models/DensNet_best_model2.h5\")\n",
    "        img_path = [\"img_path\"]\n",
    "        sex = [\"M\", \"F\"]\n",
    "        ages = [\"0's\",\"10's\",\"20's\",\"30's\",\"40's\",\"50's\",\"60+\"]\n",
    "        \n",
    "        column_names = img_path+sex+ages\n",
    "\n",
    "        # print(\"columns names : \",column_names)\n",
    "        test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "        preprocessed_data_path = root_save_path+now_path\n",
    "        # print(\"preprocessed_data_path : \",preprocessed_data_path)\n",
    "\n",
    "        img_paths = glob(preprocessed_data_path+'*.jpg')\n",
    "\n",
    "        img_names = []\n",
    "        for img_path in img_paths:\n",
    "            tmp = img_path.split(\"\\\\\")[-1]\n",
    "            img_names.append(tmp)\n",
    "\n",
    "        # print(\"img_names : \",img_names)\n",
    "        test_df = pd.DataFrame(columns=column_names)\n",
    "        test_df['img_path'] = img_names\n",
    "        # test_df.to_csv('test1.csv')\n",
    "        # print(test_df)\n",
    "        test_df = test_df.fillna(0)\n",
    "        # print(test_df.info())\n",
    "        # test_df.to_csv('test1.csv')\n",
    "\n",
    "        values = [0 for _ in range(9)]\n",
    "\n",
    "        print(\"save images 1 second...\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        test_generator=test_datagen.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=preprocessed_data_path,\n",
    "            x_col='img_path',\n",
    "            # y_col=column_names,\n",
    "            batch_size=people_num,\n",
    "            seed=777,\n",
    "            shuffle=False,\n",
    "            color_made='rgb',\n",
    "            class_mode=None,\n",
    "            target_size=(224,224))\n",
    "\n",
    "        preds = model.predict(test_generator)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        for pred in preds:\n",
    "            print(np.round(pred,3))\n",
    "            sex_pred = sex[np.argmax(pred[:2])]\n",
    "            # 주의\n",
    "            pred = pred[2:]\n",
    "            age_pred = ages[np.argmax(pred)]\n",
    "            # print(\"Dense(9,activation='sigmoid') : \", np.round(pred,3))\n",
    "            print(\"성별 예측 : \",sex_pred,\" 나이 예측 : \",age_pred)\n",
    "        \n",
    "        # 테스트\n",
    "        break\n",
    "    \n",
    "    except:\n",
    "        print(\"No one detected!\")\n",
    "        break\n",
    "\n",
    "# 웹캠 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\miniconda3\\envs\\mediapipe\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
